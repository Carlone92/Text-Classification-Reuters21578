{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Perceptron.ipynb",
      "version": "0.3.2",
      "provenance": [],
      "collapsed_sections": [],
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Carlone92/Text-Classification-Reuters21578/blob/master/Perceptron.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "GpXZT8JnzO17",
        "colab_type": "code",
        "cellView": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "from nltk import word_tokenize\n",
        "from nltk.corpus import reuters\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "import re\n",
        "from nltk.corpus import stopwords\n",
        "import nltk\n",
        "import time\n",
        "from scipy import sparse\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "\n",
        "\n",
        "def init():\n",
        "  nltk.download('reuters')\n",
        "  nltk.download('stopwords')\n",
        "  nltk.download('punkt')\n",
        "  \n",
        "def extractVocabulary(text):\n",
        "  vocabulary = set(tokenize(text))\n",
        "  return vocabulary\n",
        "\n",
        "def tokenize(text):\n",
        "  min_length = 3\n",
        "  cachedStopWords = stopwords.words(\"english\")\n",
        "  words = map(lambda word: word.lower(), word_tokenize(text))\n",
        "  words = [word for word in words if word not in cachedStopWords]\n",
        "  tokens =(list(map(lambda token: PorterStemmer().stem(token), words)))\n",
        "  p = re.compile('[a-zA-Z]+')\n",
        "  filtered_tokens = list(filter(lambda token: p.match(token) and len(token)>=min_length, tokens))\n",
        "  return filtered_tokens\n",
        "\n",
        "\n",
        "def calc_r(tfidf,train_docs):\n",
        "    return max([sparse.linalg.norm(tfidf[d]) for doc in train_docs])\n",
        "  \n",
        "\n",
        "  \n",
        "def tf(docs):\n",
        "    tf={}\n",
        "    for doc in docs:\n",
        "        words=tokenize(reuters.raw(doc))\n",
        "        doc_voc=set(words)\n",
        "        for w in doc_voc:\n",
        "            tf[(doc,w)]=words.count(w)/len(words)\n",
        "    return tf\n",
        "\n",
        "def idf(docs,sorted_voc):\n",
        "    idf=np.zeros(len(sorted_voc))\n",
        "    words=list()\n",
        "    for doc in docs:\n",
        "        word_in_doc=extractVocabulary(reuters.raw(doc))\n",
        "        words.extend(word_in_doc)\n",
        "    for word in sorted_voc:\n",
        "        idf[sorted_voc.index(word)]=math.log(len(docs))-math.log(1+words.count(word))           \n",
        "    return idf\n",
        "\n",
        "def tfidfSparse(tf,idf,doc, vocabulary):\n",
        "  tfidf=np.zeros(len(vocabulary))\n",
        "  for word in vocabulary:\n",
        "    if tf.get((doc,word),0)!=0:\n",
        "      tfidf[vocabulary.index(word)]=tf.get((doc,word))*idf[vocabulary.index(word)]\n",
        "  return sparse.csr_matrix(tfidf)\n",
        "\n",
        "def tfidf_noSparse(tf,idf,doc, vocabulary):\n",
        "  tfidf=np.zeros(len(vocabulary))\n",
        "  for word in vocabulary:\n",
        "    if tf.get((doc,word),0)!=0:\n",
        "      tfidf[vocabulary.index(word)]=tf.get((doc,word))*idf[vocabulary.index(word)]\n",
        "  return tfidf\n",
        "\n",
        "def label(d,docs_in_class):\n",
        "  if d in docs_in_class:\n",
        "    y=1\n",
        "  else:\n",
        "    y=-1\n",
        "  return y\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "6MW7mG4yOPrQ",
        "colab_type": "code",
        "cellView": "code",
        "outputId": "36546952-ea98-4fe8-a42a-d8d08681de75",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 217
        }
      },
      "cell_type": "code",
      "source": [
        "#@title Default title text\n",
        "init()\n",
        "categories='acq','earn','money-fx','grain','crude','trade'\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print(localtime)\n",
        "train_docs = [doc for c in categories for doc in reuters.fileids(c) if doc.startswith(\"train\")]\n",
        "test_docs = [doc for c in categories for doc in reuters.fileids(c) if doc.startswith(\"test\")]\n",
        "\n",
        "test_docs_in_class={cat:set(filter(lambda doc:doc in reuters.fileids(cat),test_docs)) for cat in categories}\n",
        "docs_in_class={cat:set(filter(lambda doc: doc in reuters.fileids(cat),train_docs)) for cat in categories}\n",
        "\n",
        "vocabulary=sorted(extractVocabulary(reuters.raw(train_docs)+' '))\n",
        "vocabularyTest=sorted(extractVocabulary(reuters.raw(test_docs)+' '))\n",
        "vocAll=sorted(vocabulary+vocabularyTest)\n",
        "\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print(localtime)\n",
        "  \n",
        "\n",
        "print(\"TRAIN documents:\" ,len(train_docs))\n",
        "print(\"TEST documents:\" ,len(test_docs))\n",
        "print(\"Words in vocabulary: \", len(vocabulary))"
      ],
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package reuters to /root/nltk_data...\n",
            "[nltk_data]   Package reuters is already up-to-date!\n",
            "[nltk_data] Downloading package stopwords to /root/nltk_data...\n",
            "[nltk_data]   Package stopwords is already up-to-date!\n",
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n",
            "Fri Jan 11 15:09:00 2019\n",
            "Fri Jan 11 15:10:05 2019\n",
            "TRAIN documents: 6255\n",
            "TEST documents: 2440\n",
            "Words in vocabulary:  17976\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "fvpJZtOZQ93d",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        },
        "outputId": "e65dbd86-d736-43a2-a62e-76057b1726e1"
      },
      "cell_type": "code",
      "source": [
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print('Time Start Tf',localtime)\n",
        "tfTrain=tf(train_docs)\n",
        "\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print('End Tf & Start Idf',localtime)\n",
        "\n",
        "idfTrain=idf(train_docs,vocabulary)\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print('End Idf',localtime)\n"
      ],
      "execution_count": 127,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Start Tf Fri Jan 11 15:10:05 2019\n",
            "End Tf & Start Idf Fri Jan 11 15:10:37 2019\n",
            "End Idf Fri Jan 11 15:12:50 2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "KY6PbnNgd5-6",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "63d778f2-a7fb-4c3c-89e0-9c22371b9a06"
      },
      "cell_type": "code",
      "source": [
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print('Start Tf_Idf for all documnets',localtime)\n",
        "\n",
        "tfIdfTrain={}\n",
        "for d in train_docs:\n",
        "  tfIdfTrain[d]=tfidfSparse(tfTrain,idfTrain,d,vocabulary)\n",
        "\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print('End Tf_Idf for all documnets',localtime)"
      ],
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Start Tf_Idf for all documnets Fri Jan 11 15:12:50 2019\n",
            "End Tf_Idf for all documnets Fri Jan 11 15:16:27 2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "HanI-bJrqavX",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "aad7a303-dcd5-4505-d4d9-1fe7c48c48cf"
      },
      "cell_type": "code",
      "source": [
        "r=calc_r(tfIdfTrain,train_docs)\n",
        "print(r)"
      ],
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0.47788808044290526\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "RgFH_37-KbUC",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def train(train_docs,docs_in_class,tfidf,rquadrato,n_iter):\n",
        "  \n",
        "  weight=np.zeros(len(vocabulary)+1)\n",
        "  epochs=0\n",
        "  bias=0\n",
        "  finish=False\n",
        "  while not finish and epochs < n_iter:\n",
        "    error=0\n",
        "    for d in train_docs:\n",
        "      y=label(d,docs_in_class)\n",
        "      if y*(tfidf[d].dot(weight[1:])+weight[0])<= 0:\n",
        "        weight[1:]=weight[1:]+(y*tfidf[d])\n",
        "        weight[0]=weight[0]+(y*rquadrato)\n",
        "        error+=1\n",
        "        \n",
        "    if error==0:\n",
        "      finish=True\n",
        "    epochs+=1\n",
        "  return weight[1:],weight[0],epochs\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "vj_SE2_Z5uts",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 399
        },
        "outputId": "8e6f0b7d-bc86-469c-a5a0-814b0cb18514"
      },
      "cell_type": "code",
      "source": [
        "weights={}\n",
        "epochs={}\n",
        "bias={}\n",
        "\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print(localtime)\n",
        "\n",
        "for cat in categories:\n",
        "  weights[cat],bias[cat],epochs[cat]=train(train_docs,docs_in_class[cat],tfIdfTrain,r**2,1000)\n",
        "  \n",
        "  localtime = time.asctime( time.localtime(time.time()) )\n",
        "\n",
        "  print('Categoria',cat,weights[cat])\n",
        "  print('Bias',bias[cat])\n",
        "  print('Epoche', epochs[cat],localtime)\n",
        "  \n",
        "  "
      ],
      "execution_count": 131,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Fri Jan 11 15:16:28 2019\n",
            "Categoria acq [0.         0.         0.23417205 ... 0.         0.         0.        ]\n",
            "Bias -0.22837701742940464\n",
            "Epoche 1000 Fri Jan 11 15:17:47 2019\n",
            "Categoria earn [ 0.          0.         -0.02128837 ... -0.01845869  0.\n",
            "  0.        ]\n",
            "Bias -5.551115123125783e-17\n",
            "Epoche 1000 Fri Jan 11 15:19:07 2019\n",
            "Categoria money-fx [ 0.          0.          0.         ... -0.01845869  0.\n",
            "  0.        ]\n",
            "Bias -0.2283770174294047\n",
            "Epoche 1000 Fri Jan 11 15:20:27 2019\n",
            "Categoria grain [0.         0.         0.         ... 0.03691738 0.         0.        ]\n",
            "Bias -0.2283770174294047\n",
            "Epoche 1000 Fri Jan 11 15:21:46 2019\n",
            "Categoria crude [0.        0.        0.0638651 ... 0.        0.        0.       ]\n",
            "Bias -5.551115123125783e-17\n",
            "Epoche 1000 Fri Jan 11 15:23:05 2019\n",
            "Categoria trade [0. 0. 0. ... 0. 0. 0.]\n",
            "Bias 0.22837701742940464\n",
            "Epoche 1000 Fri Jan 11 15:24:24 2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "pK9Ssp3KNNTy",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 54
        },
        "outputId": "0283ca5a-49b7-4014-fd0f-054d13a03050"
      },
      "cell_type": "code",
      "source": [
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print('Time Start Tf',localtime)\n",
        "tfTest=tf(test_docs)\n",
        "\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print('End Tf & Start Idf',localtime)\n",
        "\n",
        "idfTest=idf(test_docs,vocabulary)\n",
        "localtime = time.asctime( time.localtime(time.time()) )\n",
        "print('End Idf',localtime)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Time Start Tf Fri Jan 11 15:24:24 2019\n",
            "End Tf & Start Idf Fri Jan 11 15:24:36 2019\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "metadata": {
        "id": "PKI5w9CIIOk2",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "tfIdfTest={}\n",
        "for d in test_docs:\n",
        "  tfIdfTest[d]=tfidfSparse(tfTest,idfTest,d,vocabulary)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "fWAypp1ZMp0n",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "\n",
        "def test(docs,weights,bias,tfidf):\n",
        "  scores=np.zeros(len(docs))\n",
        "  pesi={}\n",
        "  for d in docs:\n",
        "    np.put(scores,docs.index(d),(tfidf[d].dot(weights)+bias),'raise')\n",
        "    pesi[d]=(tfidf[d].dot(weights)+bias)\n",
        "  return scores,pesi\n",
        "\n",
        "def soglia(score1):\n",
        "  score=sorted(score1)\n",
        "  soglia=np.zeros(len(score)+1)\n",
        "  i=0\n",
        "  j=2\n",
        "  while i!=len(score) and j!=len(score)+1:\n",
        "    a=np.median(score[i:j])\n",
        "    np.put(soglia,j-1,a,'raise')\n",
        "    i+=1\n",
        "    j+=1\n",
        "  soglia[0]=np.min(score)\n",
        "  soglia[-1]=np.max(score)\n",
        "  return (soglia)\n",
        "\n",
        "def calcoloTf(soglia,score,verita):\n",
        "  Tn={}\n",
        "  Tp={}\n",
        "  Fp={}\n",
        "  Fn={}\n",
        "  for s in soglia:\n",
        "    tp_count=tn_count=fn_count=fp_count=0\n",
        "    for doc in score :\n",
        "      if score[doc] >= s:\n",
        "        if doc in verita:\n",
        "          tp_count+=1\n",
        "        elif  doc not in verita:\n",
        "          fp_count+=1\n",
        "\n",
        "      else:\n",
        "        if doc not in verita:\n",
        "          fn_count+=1\n",
        "        elif doc in verita:\n",
        "          tn_count+=1\n",
        "  \n",
        "    Tn[s]=tn_count\n",
        "    Tp[s]=tp_count\n",
        "    Fp[s]=fp_count\n",
        "    Fn[s]=fn_count\n",
        "    #print('TP',Tp,'TF',Tf,'FN',Fn,'FP',Fp)\n",
        "  return Tn,Tp,Fp,Fn \n",
        "\n",
        "\n",
        "def precision_recall(soglia,tp,fn,fp):\n",
        "  precision=np.zeros(len((soglia)))\n",
        "  recall=np.zeros(len(soglia))\n",
        "  for s in range(len(soglia)):\n",
        "    if (tp[soglia[s]]+fp[soglia[s]]) !=0:\n",
        "      dp=tp[soglia[s]]/(tp[soglia[s]]+fp[soglia[s]])\n",
        "      np.put(precision,s,dp,'raise')\n",
        "    if (tp[soglia[s]]+fn[soglia[s]])!=0:\n",
        "      dr=tp[soglia[s]]/(tp[soglia[s]]+fn[soglia[s]])\n",
        "      np.put(recall,s,dr,'raise')\n",
        "  return recall,precision\n",
        "\n",
        "def Accuratezza(score,verita):\n",
        "  soglia=[0]\n",
        "  tp=tn=fp=fn=0\n",
        "  for doc in score:\n",
        "    if np.all(score[doc] < 0):\n",
        "      if doc not in verita:\n",
        "          fn+=1\n",
        "      elif doc in verita:\n",
        "          tn+=1\n",
        "        \n",
        "    else:\n",
        "      if doc in verita:\n",
        "          tp+=1\n",
        "      elif  doc not in verita:\n",
        "          fp+=1\n",
        "  div=tp+tn\n",
        "  divid=tp+tn+fp+fn\n",
        "  return np.divide(div,divid)\n",
        "\n",
        "\n",
        "def countZero(vect):\n",
        "  count=0\n",
        "  for i in range(len(vect)):\n",
        "    if vect[i] == 0:\n",
        "      print(i)\n",
        "      count+=1\n",
        "  print(count)\n",
        "\n",
        "def extractValue(dict):\n",
        "  score=np.zeros(len(dict))\n",
        "  for i in range(len(dict)):\n",
        "    np.put(score,i,dict[i],'raise')"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "aWxPtExaUy6Y",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "YD_KSw0iU3gV",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "score={}\n",
        "weights_test={}\n",
        "for cat in categories:\n",
        "  score[cat],weights_test[cat]=test(test_docs,weights[cat],bias[cat],tfIdfTest)\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "wDh8KGDcKOTM",
        "colab_type": "text"
      },
      "cell_type": "markdown",
      "source": [
        ""
      ]
    },
    {
      "metadata": {
        "id": "ijB-vF7UDGev",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        "for cat in categories:\n",
        "  t=(soglia(score[cat]))\n",
        "  print(t)\n",
        "  tf,tp,fp,fn=calcoloTf(t,weights_test[cat],test_docs_in_class[cat])\n",
        "  recall,precision=precision_recall(t,tp,fn,fp)\n",
        "  plt.xlabel('Recall')\n",
        "  plt.ylabel('Precision')\n",
        "  plt.plot(recall,precision)\n",
        "  print(cat)\n",
        "  plt.show()\n",
        "   "
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "metadata": {
        "id": "MRWAMAyzOGwd",
        "colab_type": "code",
        "colab": {}
      },
      "cell_type": "code",
      "source": [
        ""
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}